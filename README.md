# ğŸ§  Neural Network from Scratch | XOR Problem Solver ğŸ¤–  
![Python](https://img.shields.io/badge/Python-3.6+-blue?logo=python) ![NumPy](https://img.shields.io/badge/NumPy-Numerical%20Computing-blue?logo=numpy) ![License](https://img.shields.io/badge/License-MIT-yellow) ![Status](https://img.shields.io/badge/Status-Active-brightgreen)

<p align="center">
  <img src="https://cdn-icons-png.flaticon.com/512/2103/2103633.png" alt="Neural Network Logo" width="140"/>
</p>

ğŸš€ A minimal **neural network implementation from scratch** in Python using only NumPy. This project demonstrates the fundamental mechanics of neural networks by solving the **XOR problem** through backpropagation and gradient descent. Perfect for understanding the inner workings of deep learning without highâ€‘level frameworks.

---

## âœ¨ Key Features  
ğŸ§  **Pure Python & NumPy** â€“ No deep learning frameworks, just maths  
âš¡ **Forward & Backward Pass** â€“ Complete propagation implementation  
ğŸ“‰ **Loss Monitoring** â€“ Tracks mean squared error during training  
ğŸ¯ **XOR Solution** â€“ Successfully learns the nonâ€‘linear XOR function  
ğŸ”§ **Customizable Architecture** â€“ Adjust hidden neurons, learning rate, epochs  
ğŸ“š **Educational Code** â€“ Clear, wellâ€‘commented code for learning  

---

## ğŸ§  Tech Stack  
- **Language:** Python ğŸ  
- **Numerical Library:** NumPy ğŸ”¢  
- **Activation:** Sigmoid  
- **Loss Function:** Mean Squared Error  
- **Optimizer:** Gradient Descent  

---

## ğŸ“¦ Installation  

```bash
git clone https://github.com/SayabArshad/neural-network-from-scratch.git
cd neural-network-from-scratch
pip install numpy
```
---

## â–¶ï¸ Usage

Run the main script:

```bash
python basic_neural_network.py
```
Youâ€™ll see the training loss over epochs and the final predictions for all XOR input combinations.

---

## ğŸ“ Project Structure

```
neural-network-from-scratch/
â”‚-- basic_neural_network.py          # Main implementation
â”‚-- requirements.txt                  # Dependencies (numpy)
â”‚-- README.md                         # Documentation
â”‚-- assets/                           # Images for README
â”‚    â”œâ”€â”€ code.JPG
â”‚    â””â”€â”€ output.JPG
```

---

## ğŸ–¼ï¸ Interface Previews

|       ğŸ“ Code Snippet       |        ğŸ“Š Console Output        |
| :-------------------------: | :-----------------------------: |
| ![Code Snippet](assets/code.JPG) | ![Output](assets/output.JPG) |

---

## ğŸ’¡ About the Project

This project builds a feedforward neural network with one hidden layer from absolute scratch. It uses the sigmoid activation function, mean squared error loss, and gradient descent with backpropagation. The XOR problem is a classic benchmark because it is not linearly separable, making it an ideal test for a neural networkâ€™s capacity to learn nonâ€‘linear patterns. By implementing everything manually, you gain deep insight into how networks really work under the hood.

---

## ğŸ§‘â€ğŸ’» Author


**Developed by:** [Sayab Arshad Soduzai](https://github.com/SayabArshad) ğŸ‘¨â€ğŸ’»

ğŸ“… **Version:** 1.0.0

ğŸ“œ **License:** MIT License

---

## â­ Contributions

Contributions are welcome! Fork the repository, open issues, or submit pull requests to add features like:

More hidden layers

Different activation functions (ReLU, tanh)

Batch training

Momentum optimization
If you find this project helpful, please â­ star the repository to show your support.

---

## ğŸ“§ Contact

For queries, collaborations, or feedback, reach out at **[sayabarshad789@gmail.com](mailto:sayabarshad789@gmail.com)**

---

ğŸ§  Understanding neural networks one line of code at a time.
